{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"E:\\Course\\Deep Learning Computer Vision™ CNN, OpenCV, YOLO, SSD & GANs\\12. Types of Optimizers, Learning Rates & Callbacks Build a Fruit Classifier\\fruits-360\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Course\\\\Deep Learning Computer Vision™ CNN, OpenCV, YOLO, SSD & GANs\\\\12. Types of Optimizers, Learning Rates & Callbacks Build a Fruit Classifier\\\\fruits-360'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_img_dim  = (32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = os.getcwd()+'\\\\'+os.listdir()[-2]\n",
    "validation = os.getcwd()+'\\\\'+os.listdir()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Flatten, Dense, Convolution2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gen = ImageDataGenerator(rotation_range = 20, width_shift_range = 0.1, height_shift_range = 0.1, shear_range = 0.1, horizontal_flip = True, rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41322 images belonging to 81 classes.\n",
      "Found 13877 images belonging to 81 classes.\n"
     ]
    }
   ],
   "source": [
    "train_img_gen = img_gen.flow_from_directory(train, target_size = std_img_dim[:2], batch_size = 10)\n",
    "validation_img_gen = img_gen.flow_from_directory(validation, target_size = std_img_dim[:2], shuffle = False, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = std_img_dim))\n",
    "model.add(Convolution2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = std_img_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = std_img_dim))\n",
    "model.add(Convolution2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = std_img_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(512, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(81, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'accuracy', patience = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4133/4133 [==============================] - 1108s 268ms/step - loss: 0.8650 - accuracy: 0.7437 - val_loss: 0.3142 - val_accuracy: 0.9089\n",
      "Epoch 2/10\n",
      "4133/4133 [==============================] - 394s 95ms/step - loss: 0.2656 - accuracy: 0.9240 - val_loss: 0.2756 - val_accuracy: 0.9189\n",
      "Epoch 3/10\n",
      "4133/4133 [==============================] - 248s 60ms/step - loss: 0.2452 - accuracy: 0.9395 - val_loss: 0.4696 - val_accuracy: 0.9235\n",
      "Epoch 4/10\n",
      "4133/4133 [==============================] - 240s 58ms/step - loss: 0.2530 - accuracy: 0.9478 - val_loss: 0.3283 - val_accuracy: 0.9439\n",
      "Epoch 5/10\n",
      "4133/4133 [==============================] - 240s 58ms/step - loss: 0.3096 - accuracy: 0.9464 - val_loss: 0.4550 - val_accuracy: 0.9431\n",
      "Epoch 6/10\n",
      "4133/4133 [==============================] - 239s 58ms/step - loss: 0.3672 - accuracy: 0.9474 - val_loss: 0.3496 - val_accuracy: 0.9555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2166ef6b670>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_img_gen, batch_size = 32, epochs = 10, validation_data = validation_img_gen, callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(model.predict(validation_img_gen), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 80, 80, 80], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.84      0.74       164\n",
      "           1       0.99      0.96      0.98       164\n",
      "           2       0.95      1.00      0.97       164\n",
      "           3       0.83      0.99      0.91       161\n",
      "           4       0.99      0.99      0.99       164\n",
      "           5       0.95      0.96      0.96       164\n",
      "           6       0.94      0.82      0.88       164\n",
      "           7       1.00      0.92      0.96       144\n",
      "           8       1.00      1.00      1.00       166\n",
      "           9       0.97      0.96      0.96       164\n",
      "          10       1.00      0.90      0.95       164\n",
      "          11       1.00      1.00      1.00       143\n",
      "          12       1.00      1.00      1.00       166\n",
      "          13       0.96      0.82      0.88       166\n",
      "          14       0.90      0.92      0.91       166\n",
      "          15       0.77      1.00      0.87       166\n",
      "          16       1.00      1.00      1.00       164\n",
      "          17       1.00      0.99      1.00       164\n",
      "          18       0.92      0.90      0.91       166\n",
      "          19       0.70      0.65      0.67       164\n",
      "          20       0.72      0.82      0.77       246\n",
      "          21       0.99      0.85      0.91       246\n",
      "          22       1.00      1.00      1.00       164\n",
      "          23       1.00      1.00      1.00       164\n",
      "          24       1.00      1.00      1.00       164\n",
      "          25       1.00      1.00      1.00       166\n",
      "          26       1.00      0.97      0.98       166\n",
      "          27       1.00      0.99      1.00       166\n",
      "          28       0.98      0.99      0.99       166\n",
      "          29       0.99      0.99      0.99       164\n",
      "          30       1.00      1.00      1.00       166\n",
      "          31       1.00      1.00      1.00       166\n",
      "          32       1.00      0.92      0.96       166\n",
      "          33       0.99      1.00      1.00       164\n",
      "          34       1.00      1.00      1.00       166\n",
      "          35       0.99      1.00      1.00       166\n",
      "          36       0.98      1.00      0.99       166\n",
      "          37       1.00      0.99      1.00       156\n",
      "          38       0.99      0.99      0.99       166\n",
      "          39       0.99      0.94      0.97       164\n",
      "          40       1.00      0.97      0.98       166\n",
      "          41       1.00      0.99      1.00       166\n",
      "          42       1.00      0.96      0.98       166\n",
      "          43       0.96      0.99      0.97       166\n",
      "          44       1.00      1.00      1.00       166\n",
      "          45       0.99      0.90      0.94       166\n",
      "          46       1.00      1.00      1.00       246\n",
      "          47       1.00      1.00      1.00       164\n",
      "          48       0.83      0.79      0.81       164\n",
      "          49       0.99      1.00      1.00       160\n",
      "          50       1.00      0.99      0.99       164\n",
      "          51       0.99      1.00      0.99       166\n",
      "          52       0.97      0.74      0.84       164\n",
      "          53       0.82      0.96      0.88       164\n",
      "          54       0.99      0.84      0.91       164\n",
      "          55       0.85      0.99      0.91       166\n",
      "          56       0.95      1.00      0.98       166\n",
      "          57       1.00      0.98      0.99       166\n",
      "          58       0.78      0.88      0.83       166\n",
      "          59       1.00      0.99      1.00       164\n",
      "          60       0.97      0.98      0.98       164\n",
      "          61       0.98      1.00      0.99       166\n",
      "          62       1.00      0.98      0.99       163\n",
      "          63       0.97      1.00      0.98       166\n",
      "          64       0.99      1.00      1.00       151\n",
      "          65       0.95      0.79      0.86       164\n",
      "          66       0.99      1.00      1.00       166\n",
      "          67       0.99      1.00      0.99       164\n",
      "          68       1.00      1.00      1.00       166\n",
      "          69       0.83      0.99      0.91       162\n",
      "          70       0.98      0.98      0.98       164\n",
      "          71       0.99      0.89      0.94       246\n",
      "          72       1.00      0.99      0.99       166\n",
      "          73       0.94      0.99      0.96       166\n",
      "          74       0.99      0.98      0.99       246\n",
      "          75       0.95      1.00      0.98       225\n",
      "          76       1.00      0.96      0.98       246\n",
      "          77       1.00      0.99      1.00       160\n",
      "          78       1.00      1.00      1.00       164\n",
      "          79       1.00      1.00      1.00       127\n",
      "          80       1.00      1.00      1.00       249\n",
      "\n",
      "    accuracy                           0.96     13877\n",
      "   macro avg       0.96      0.96      0.96     13877\n",
      "weighted avg       0.96      0.96      0.96     13877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_img_gen.classes, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Course\\\\Deep Learning Computer Vision™ CNN, OpenCV, YOLO, SSD & GANs\\\\12. Types of Optimizers, Learning Rates & Callbacks Build a Fruit Classifier\\\\fruits-360'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"D:\\B2B_Git_Instance\\CNN_Keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fruit_classifier.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
